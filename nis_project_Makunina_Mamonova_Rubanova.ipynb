{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проект по НИСу \"Не мой язык\". Настя Макунина, Таня Мамонова, Женя Рубанова"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нужно было повторить эксперимент, описанный в статье Dickinson, Herring (2008) -- использовать FSA для локализации ошибки в слове "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чистим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('morphdict_verb.csv', 'r', encoding = 'utf-8') as c:\n",
    "    bad_dict = csv.reader(c, delimiter=';')\n",
    "    bad_dict = list(bad_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lemma',\n",
       " 'Morph',\n",
       " 'Status',\n",
       " 'Plaсе',\n",
       " 'Allomorph',\n",
       " 'POS',\n",
       " 'CommentWord',\n",
       " 'CommentMorph']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbad_dict = bad_dict[1:]\\nverb_marks = ['ь', 'ся', 'ти']\\nfor row in bad_dict:\\n    for mark in verb_marks:\\n        if row[0].endswith(mark):\\n            d.append(row)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = []\n",
    "d = bad_dict[1:]\n",
    "\n",
    "\"\"\"\n",
    "bad_dict = bad_dict[1:]\n",
    "verb_marks = ['ь', 'ся', 'ти']\n",
    "for row in bad_dict:\n",
    "    for mark in verb_marks:\n",
    "        if row[0].endswith(mark):\n",
    "            d.append(row)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = [x[:5] for x in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = [[x[0], x[1], x[2], x[3], re.sub('[0-9]', '', x[4])] for x in d]\n",
    "d = [[x[0], x[1], x[2], x[3], re.sub('\\(.*\\)', '', x[4])] for x in d]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сначала мы решили сделать FSA, но он не считал ошибками случаи типа \"покуреть\", т.е. когда суффикс существует, но в конкретном слове он неверен"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поэтому мы сделали граф, который будет определять, может ли та или иная морфема быть в слове. На вход подается словарь, мы сравниваем в каждой строке нулевой элемент словаря - слово, если он совпадает с последующим, то мы добавляем первый элемент, который содержит в себе морфему в граф. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_graph(d, restricted_morphs={\"0\", \"\", None}):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    def add_morph_edge(morph1, morph2):\n",
    "        if all((morph1, morph2 not in restricted_morphs)):\n",
    "            G.add_edge(morph1, morph2)\n",
    "    \n",
    "    start_word = \"<START>\"\n",
    "    end_word = \"<END>\"\n",
    "    prev_word = None\n",
    "    prev_morph = None\n",
    "    d.append([None, None])\n",
    "    \n",
    "    for el in d:\n",
    "        word = el[0]\n",
    "        morph = el[1]\n",
    "        if prev_word != word:\n",
    "            add_morph_edge(prev_morph, end_word)\n",
    "            add_morph_edge(start_word, morph)\n",
    "            prev_word = word\n",
    "        else:\n",
    "            add_morph_edge(prev_morph, morph)\n",
    "        prev_morph = morph\n",
    "    \n",
    "    d.pop()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 584 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "G = make_graph(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(G.has_edge(\"кур\", \"и\"))\n",
    "print(G.has_edge(\"кур\", \"е\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## делаем forward - обычный FSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сначала сделаем списки морфем, которые встретились в нашем словаре и добавим аффиксы, которых там нет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roots = []\n",
    "suffs = []\n",
    "prefs = []\n",
    "flex = []\n",
    "\n",
    "for row in d:\n",
    "    if row[2] == 'суффикс':\n",
    "        suffs.append(row[1])\n",
    "    elif row[2] == 'префикс':\n",
    "        prefs.append(row[1])\n",
    "    elif row[2] == 'флексия':\n",
    "        flex.append(row[1])\n",
    "    elif row[2] == 'корень':\n",
    "        if row[4] != '':\n",
    "            arr = row[4].split('|')\n",
    "            arr = [re.sub('[\\[\\]]', '', x) for x in arr]\n",
    "            for i in arr:\n",
    "                roots.append(i)\n",
    "        else:\n",
    "            roots.append(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefs = set(prefs)\n",
    "roots = set(roots)\n",
    "suffs = set(suffs)\n",
    "flex = set(flex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во префиксов:  90\n",
      "Кол-во суффиксов:  171\n",
      "Кол-во окончаний:  7\n",
      "Кол-во корней:  3180\n"
     ]
    }
   ],
   "source": [
    "print('Кол-во префиксов: ', len(prefs))\n",
    "print('Кол-во суффиксов: ', len(suffs))\n",
    "print('Кол-во окончаний: ', len(flex))\n",
    "print('Кол-во корней: ', len(roots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flex.remove('ь')\n",
    "suffs.remove('ь')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flex_to_add = [\"ем\", \"ешь\", \"ете\", \"ет\", \"ут\", \"ют\", \"у\", \"ю\", \"им\", \"ишь\", \"ите\", \"ит\", \"ат\", \"ят\", \"а\", \"и\", \"о\", \"сь\"]\n",
    "\n",
    "for f in flex_to_add:\n",
    "    flex.add(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffs_to_add = [\"ова\", \"ева\", \"ива\", 'ирова', \"ыва\", \"а\", \"и\", \"я\", \"е\", \"л\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "suffs = ['а', 'я', 'к', 'а', 'е', 'ев', 'а', 'ов', 'а', 'и', 'нич', 'а', 'ну', 'ств', 'ов', 'а'] + suffs_to_add\n",
    "suffs = set(suffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prefs = {'взо', 'во', 'в', 'до', 'воз', 'вз', 'вс', 'вы', 'возо', 'за', 'изо', 'ко', 'над', 'надо', 'на', 'недо', 'о', 'об', 'обо', 'от', 'пере', 'по', 'под', 'пред', 'про', 'с', 'со', 'среди', 'через', 'черес', 'раз' ,'рас', 'роз', 'рос', 'без', 'бес', 'оз', 'вос', 'из', 'ис', 'низ', 'нис', 'пре', 'при', 'от', 'ото'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roots.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roots.add('блок')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'' in flex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Строим автомат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateMachine:\n",
    "    def __init__(self):\n",
    "        self.handlers = {}\n",
    "        self.startState = None\n",
    "        self.endStates = []\n",
    "\n",
    "    def add_state(self, name, handler, end_state=0):\n",
    "        name = name.upper()\n",
    "        self.handlers[name] = handler\n",
    "        if end_state:\n",
    "            self.endStates.append(name)\n",
    "\n",
    "    def set_start(self, name):\n",
    "        self.startState = name.upper()\n",
    "\n",
    "    def run(self, cargo):\n",
    "        try:\n",
    "            handler = self.handlers[self.startState]\n",
    "        except:\n",
    "            raise InitializationError(\"must call .set_start() before .run()\")\n",
    "        if not self.endStates:\n",
    "            raise  InitializationError(\"at least one state must be an end_state\")\n",
    "    \n",
    "        while True:\n",
    "            (newState, cargo) = handler(cargo)\n",
    "            if newState.upper() in self.endStates:\n",
    "                print(\"reached \", newState)\n",
    "                break \n",
    "            else:\n",
    "                handler = self.handlers[newState.upper()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#suffs\n",
    "#prefs\n",
    "#roots\n",
    "#flex\n",
    "\n",
    "analyze = ''\n",
    "smth = False\n",
    "\n",
    "## На вход подаем слово, начинаем сопоставлять с префиксами из нашего списка, проверяем на графе, оставшееся записываем в rest\n",
    "\n",
    "def pref_transitions(word):\n",
    "    global G\n",
    "    found = []\n",
    "    rest = []\n",
    "    for i in prefs:\n",
    "        if re.match(i, word):\n",
    "            found.append(re.match(i, word).group(0))\n",
    "    if len(found) > 0:\n",
    "        for f in found:\n",
    "            if G.has_edge(\"<START>\", f):\n",
    "                #приставка, остаток\n",
    "                rest.append([f, word[len(f):]])\n",
    "    if len(rest) == 0:\n",
    "        rest = [word]\n",
    "    newState = 'Pref_state'\n",
    "    return (newState, rest)\n",
    "\n",
    "## На вход подаем оставшееся после префикса, начинаем сопоставлять с корнями и с графом, оставшееся записываем в rest\n",
    "\n",
    "def root_transitions(rest):\n",
    "    found = []\n",
    "    #print(rest)\n",
    "    for item in rest:\n",
    "        for root in roots:\n",
    "            if isinstance(item, list): \n",
    "                if re.match(root, item[1]):\n",
    "                    if G.has_edge(item[0], root):\n",
    "                        #приставка, корень, остаток\n",
    "                        found.append([item[0], re.match(root, item[1]).group(0), item[1][len(root):]])\n",
    "            else:\n",
    "                #print(item)\n",
    "                if re.match(root, item):\n",
    "                    if G.has_edge(\"<START>\", root):\n",
    "                        #корень, остаток\n",
    "                        found.append([re.match(root, item).group(0), item[len(root):]])\n",
    "    if len(found) > 0:\n",
    "        newState = 'Root_state'\n",
    "        new_rest = []\n",
    "        for f in found:\n",
    "            new_rest.append(f)\n",
    "        rest = new_rest\n",
    "    else:\n",
    "        newState = 'PrefRoot_Error'\n",
    "    return (newState, rest)\n",
    "\n",
    "\n",
    "def suff_transitions(rest):\n",
    "    stop = False\n",
    "    while stop == False:\n",
    "        rest, stop = find_morphs(rest, suffs)\n",
    "    newState = 'Suff_state'\n",
    "    return (newState, rest)\n",
    "        \n",
    "\n",
    "## Смотрим, что у нас осталось???????    \n",
    "    \n",
    "def find_morphs(rest, morphs):\n",
    "    global smth\n",
    "    found = []\n",
    "    stop = False\n",
    "    for item in rest:\n",
    "        for s in morphs:\n",
    "            if re.match(s, item[-1]):\n",
    "                if G.has_edge(item[-2], s):\n",
    "                    #print('в сказку попал')\n",
    "                    #print(s)\n",
    "                    #(приставка), корень, суффикс, остаток\n",
    "                    arr = [item[:-1], [re.match(s, item[-1]).group(0)], [item[-1][len(s):]]]\n",
    "                    found.append([item for sublist in arr for item in sublist])\n",
    "                    smth = True\n",
    "    if found == []:\n",
    "        stop = True\n",
    "        found = rest\n",
    "    #print(found)\n",
    "    \n",
    "    return found, stop\n",
    "\n",
    "\n",
    "def flex_transitions(rest):\n",
    "    stop = False\n",
    "    #print('ОКОНЧАНИЯ')\n",
    "    while stop == False:\n",
    "        rest, stop = find_morphs(rest, flex)\n",
    "    #print(rest)\n",
    "    global analyze\n",
    "    analyze = rest\n",
    "    if analyze[0][-1] != '':\n",
    "        newState = 'SufFlex_Error'\n",
    "    else:\n",
    "        newState = 'Flex_state'\n",
    "    return (newState, rest)\n",
    "    \n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    m_f = StateMachine()\n",
    "    m_f.add_state(\"Start\", pref_transitions)\n",
    "    m_f.add_state(\"Pref_state\", root_transitions)\n",
    "    m_f.add_state(\"Root_state\", suff_transitions)\n",
    "    m_f.add_state(\"Suff_state\", flex_transitions)\n",
    "    m_f.add_state(\"Flex_state\", None, end_state=1)\n",
    "    m_f.add_state(\"PrefRoot_Error\", None, end_state=1)\n",
    "    m_f.add_state(\"SufFlex_Error\", None, end_state=1) \n",
    "    m_f.set_start(\"Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вот как он работает: на вход поступает слово, если оно написано правильно, то автомат достигает Flex_State, если нет, то состояния ошибки с указанием морфемы, в которой ошибка допущена"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  Flex_state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['по', 'беж', 'а', 'ть', '']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_f.run(\"побежать\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  SufFlex_Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['по', 'беж', 'а', 'тб']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_f.run(\"побежатб\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  SufFlex_Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['по', 'беж', 'ить']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_f.run(\"побежить\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  SufFlex_Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['о', 'н', 'е', 'сти']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_f.run(\"онести\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## теперь back - обратный FSA. Он построен по аналогии с прямым автоматом, только анализ слова теперь будет начинаться с окончания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "back_flex = set([x[::-1] for x in flex])\n",
    "back_roots = set([x[::-1] for x in roots])\n",
    "back_suffs = set([x[::-1] for x in suffs])\n",
    "back_prefs = set([x[::-1] for x in prefs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#suffs\n",
    "#prefs\n",
    "#roots\n",
    "#flex\n",
    "\n",
    "analyze = ''\n",
    "\n",
    "def back_flex_transitions(rest):\n",
    "    stop = False\n",
    "    rest = rest[::-1]\n",
    "    found = []\n",
    "    for f in back_flex:\n",
    "        if re.match(f, rest):\n",
    "            if G.has_edge(f[::-1], \"<END>\"):\n",
    "                found.append([f, rest[len(f):]])\n",
    "    \n",
    "    rest, stop = back_find_morphs(found, back_flex)\n",
    "            \n",
    "    newState = 'Flex_state'\n",
    "    return (newState, rest)\n",
    "\n",
    "def back_find_morphs(rest, morphs):\n",
    "    #print(rest)\n",
    "    found = []\n",
    "    stop = False\n",
    "    for item in rest:\n",
    "        for s in morphs:\n",
    "            if re.match(s, item[-1]):\n",
    "                if G.has_edge(item[-2][::-1], s[::-1]):\n",
    "                    #print('в сказку попал')\n",
    "                    #print(s)\n",
    "                    #(приставка), корень, суффикс, остаток\n",
    "                    arr = [item[:-1], [re.match(s, item[-1]).group(0)], [item[-1][len(s):]]]\n",
    "                    found.append([item for sublist in arr for item in sublist])\n",
    "                    #print(found)\n",
    "    if found == []:\n",
    "        stop = True\n",
    "        found = rest\n",
    "    return found, stop\n",
    "\n",
    "def back_suff_transitions(rest):\n",
    "    stop = False\n",
    "    while stop == False:\n",
    "        rest, stop = find_morphs(rest, back_suffs)\n",
    "    newState = 'Suff_state'\n",
    "    return (newState, rest)\n",
    "\n",
    "def back_root_transitions(rest):\n",
    "    global analyze\n",
    "    found = []\n",
    "    for item in rest:\n",
    "        for root in back_roots:\n",
    "            if isinstance(item, list): \n",
    "                if re.match(root, item[-1]):\n",
    "                    if G.has_edge(item[-2][::-1], root[::-1]):\n",
    "                        arr = [item[:-1], [re.match(root, item[-1]).group(0)], [item[-1][len(root):]]]\n",
    "                        found.append([item for sublist in arr for item in sublist])\n",
    "            else:\n",
    "                #print(item)\n",
    "                if re.match(root, item):\n",
    "                    if G.has_edge(\"<START>\", root[::-1]):\n",
    "                        #корень, остаток\n",
    "                        found.append([re.match(root, item).group(0), item[len(root):]])\n",
    "    if len(found) > 0:\n",
    "        newState = 'Root_state'\n",
    "        new_rest = []\n",
    "        for f in found:\n",
    "            new_rest.append(f)\n",
    "        rest = new_rest\n",
    "    else:\n",
    "        analyze = [[el[::-1] for el in reversed(var)] for var in rest]\n",
    "        newState = 'SufFlex_Error'\n",
    "    return (newState, rest)\n",
    "\n",
    "\n",
    "\n",
    "def back_pref_transitions(rest):\n",
    "    global analyze\n",
    "    found = []\n",
    "    for item in rest:\n",
    "        for p in back_prefs:\n",
    "            if re.match(p, item[-1]):\n",
    "                if G.has_edge(item[-2][::-1], p[::-1]):\n",
    "                    arr = [item[:-1], [re.match(p, item[-1]).group(0)], [item[-1][len(p):]]]\n",
    "                    found.append([item for sublist in arr for item in sublist])\n",
    "    if not found:\n",
    "        newState = 'PrefRoot_Error'\n",
    "    else:\n",
    "        newState = 'Pref_state'\n",
    "    analyze = [[el[::-1] for el in reversed(var)] for var in rest]\n",
    "    return (newState, analyze)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    m_b = StateMachine()\n",
    "    m_b.add_state(\"Start\", back_flex_transitions)\n",
    "    m_b.add_state(\"Flex_state\", back_suff_transitions)\n",
    "    m_b.add_state(\"Suff_state\", back_root_transitions)\n",
    "    m_b.add_state(\"Root_state\", back_pref_transitions)\n",
    "    m_b.add_state(\"Pref_state\", None, end_state=1)\n",
    "    m_b.add_state(\"PrefRoot_Error\", None, end_state=1)\n",
    "    m_b.add_state(\"SufFlex_Error\", None, end_state=1) \n",
    "    m_b.set_start(\"Start\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вот как он работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  Pref_state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['побе', 'ж', 'а', 'ть'], ['по', 'беж', 'а', 'ть']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_b.run(\"побежать\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  SufFlex_Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_b.run(\"побежатб\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  Pref_state\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['очер', 'т', 'и'], ['о', 'черт', 'и']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_b.run(\"очерти\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  PrefRoot_Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['зчер', 'т', 'и'], ['з', 'черт', 'и']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_b.run(\"зчерти\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  PrefRoot_Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['о', 'нес', 'ти'], ['онес', 'т', 'и']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_b.run(\"онести\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  PrefRoot_Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['побе', 'ж', 'и', 'ть'], ['поб', 'еж', 'и', 'ть']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_b.run(\"побежить\".lower())\n",
    "analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравниваем выдачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  SufFlex_Error\n",
      "reached  PrefRoot_Error\n"
     ]
    }
   ],
   "source": [
    "m_f.run(\"побежить\".lower())\n",
    "m_b.run(\"побежить\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached  Flex_state\n",
      "reached  Pref_state\n"
     ]
    }
   ],
   "source": [
    "m_f.run(\"побежать\".lower())\n",
    "m_b.run(\"побежать\".lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что в одном и том же слове в зависимости от того, каким именно автоматом его анализировать, будет разная локализация ошибок. Например, если в слове есть правильная (которая есть в словаре), но не подходящая к корню приставка, то эту ошибку будет детектить только обратный автомат. Соответстенно, наоборот, если то же самое сделать с окончанием, то будет детектить прямой автомат."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
